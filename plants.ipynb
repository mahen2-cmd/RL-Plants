{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mesa\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Development of an agent-based simulation model in combination with reinforcement learning in Python using Mesa library\n",
    "\n",
    "\n",
    "\n",
    "# - At the beginning of an episode, 10 plants (as agents) are planted\n",
    "\n",
    "# - Plants must grow for 10 days (steps) before they can be harvested.\n",
    "\n",
    "# - Each plant has a 10% chance of dying every day.\n",
    "\n",
    "# - A new (fresh) plant can be bought every day (cost $10) to be planted\n",
    "\n",
    "# - The aim is to harvest 10 plants that each grew for 10 days. When the goal is reached, there is a reward of $20 per plant harvested and the episode ends\n",
    "\n",
    "# - Each day of the episode costs $5\n",
    "\n",
    "# - Reinforcement learning is now used to find a strategy when to plant new trees that minimizes total costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlantAgent(mesa.Agent):\n",
    "\n",
    "    def __init__(self, unique_id, model):\n",
    "        super().__init__(unique_id, model)\n",
    "\n",
    "        self.alive = True\n",
    "        self.age = 0\n",
    "        self.harvested = False\n",
    "\n",
    "    def step(self):\n",
    "        if self.alive and self.age != 10:\n",
    "            self.age += 1\n",
    "\n",
    "        if self.age == 10:\n",
    "            self.harvested = True\n",
    "\n",
    "\n",
    "        if not self.alive or self.harvested:\n",
    "            return\n",
    "\n",
    "\n",
    "        death_outcome = random.choices([False, True], weights=[0.1, 0.9], k=1)[0]\n",
    "        self.alive = death_outcome\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlantModel(mesa.Model):\n",
    "\n",
    "    def __init__(self, N):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_agents = N\n",
    "        self.number_of_days = 0\n",
    "        self.number_of_plants_harvested = 0\n",
    "\n",
    "        self.schedule = mesa.time.RandomActivation(self)\n",
    "\n",
    "        for i in range(self.num_agents):\n",
    "            a = PlantAgent(i, self)\n",
    "            self.schedule.add(a)\n",
    "\n",
    "    def get_state(self):\n",
    "        agents = [a for a in self.schedule.agents]\n",
    "        state = []\n",
    "\n",
    "        number_of_plants_harvested = 0\n",
    "\n",
    "        for a in agents:\n",
    "\n",
    "            if a.alive and not a.harvested:\n",
    "                state.append(a.age)\n",
    "\n",
    "            if a.harvested:\n",
    "                number_of_plants_harvested += 1\n",
    "\n",
    "\n",
    "        return number_of_plants_harvested, tuple(sorted(state))\n",
    "\n",
    "    def check_terminated(self, observation):\n",
    "        return list(map(lambda x: x[1] == 10, observation)).count(True)\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        self.number_of_days += 1\n",
    "        terminated = 0\n",
    "        truncated = False\n",
    "        reward = -5\n",
    "\n",
    "        if action == 1:\n",
    "            self.num_agents += 1\n",
    "            a = PlantAgent(self.num_agents - 1, self)\n",
    "            self.schedule.add(a)\n",
    "\n",
    "            reward -= 10\n",
    "\n",
    "        self.schedule.step()\n",
    "\n",
    "        observation = self.get_state()\n",
    "\n",
    "\n",
    "        difference = observation[0] - self.number_of_plants_harvested\n",
    "        reward += difference * 20\n",
    "\n",
    "        self.number_of_plants_harvested = observation[0]\n",
    "\n",
    "        if self.number_of_days == 500:\n",
    "            truncated = True\n",
    "\n",
    "        if self.number_of_plants_harvested == 10:\n",
    "            terminated = 1\n",
    "\n",
    "        return observation, reward, terminated, truncated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thera are a few things to consider\n",
    "\n",
    "Terminal State: This would be achieved when we have 10 plants harvested.  \n",
    "Truncation: The environment would truncate in 500 steps.  \n",
    "Reward: Positive for harvesting and negative for buying a plant and on every step.  \n",
    "Observation: This would be the current state. The state would be the number of plants harvested and the age of each plant.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN THIS - THIS is JUST TO ESTIMATE THE STATE SPACE\n",
    "\n",
    "def get_state_space(number_of_iterations = 100000):\n",
    "\n",
    "    state_space = set()\n",
    "\n",
    "    truncated = 0\n",
    "    model = PlantModel(10)\n",
    "    initial_state = model.get_state()\n",
    "    state_space.add(initial_state)\n",
    "\n",
    "\n",
    "    for _ in tqdm(range(number_of_iterations)):\n",
    "\n",
    "        model = PlantModel(10)\n",
    "        observation = []\n",
    "        terminated = 0\n",
    "\n",
    "\n",
    "\n",
    "        while not terminated and not truncated:\n",
    "\n",
    "            action = random.randint(0, 1)\n",
    "            observation, _, terminated, truncated = model.step(action)\n",
    "            state_space.add(observation)\n",
    "\n",
    "    state_space = list(state_space)\n",
    "    state_space = sorted(state_space, key=lambda x: (x[0], x[1]))\n",
    "\n",
    "    return state_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN\n",
    "# state_space = get_state_space(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN THIS - THIS IS JUST TO STORE THE STATE SPACE\n",
    "\n",
    "\n",
    "# with open(\"state_space.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(state_space, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"state_space.pkl\", \"rb\") as file:\n",
    "    state_space = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_Q_Table(state_space):\n",
    "    Q_Table = {}\n",
    "\n",
    "    for s in state_space:\n",
    "        Q_Table[s] = [0, 0]\n",
    "\n",
    "    return Q_Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_Table = initialize_Q_Table(state_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trajectory(policy):\n",
    "\n",
    "    trajectory = []\n",
    "\n",
    "    state = (0, (0, 0, 0, 0, 0, 0, 0, 0, 0, 0))\n",
    "    terminated = 0\n",
    "    truncated = 0\n",
    "    next_state = (0, (0, 0, 0, 0, 0, 0, 0, 0, 0, 0))\n",
    "\n",
    "    model = PlantModel(10)\n",
    "\n",
    "    while not terminated and not truncated:\n",
    "        state = next_state\n",
    "        action = policy[state]\n",
    "\n",
    "        observation, reward, terminated, truncated = model.step(action)\n",
    "\n",
    "        next_state = observation\n",
    "        experience_tuple = (state, action, reward, next_state)\n",
    "\n",
    "        trajectory.append(experience_tuple)\n",
    "\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning:\n",
    "    def __init__(self, Q_Table, gamma=1, number_of_iterations=100000):\n",
    "        self.number_of_iterations = number_of_iterations\n",
    "\n",
    "        self.Q = Q_Table\n",
    "\n",
    "        self.ε = self.get_parameters_exponential_decay(decay_rate=0.999995)\n",
    "        self.α = self.get_parameters_exponential_decay(decay_rate=0.999995)\n",
    "\n",
    "        self.trajectories = [[]]\n",
    "\n",
    "    def epsilon_greedy_exponential(self, iteration, s):\n",
    "        ε = self.ε[iteration]\n",
    "\n",
    "        a = 0\n",
    "\n",
    "        if s not in self.Q:\n",
    "            self.Q[s] = [0, 0]\n",
    "\n",
    "        if np.random.random() > ε:\n",
    "            a = np.argmax(self.Q[s])\n",
    "        else:\n",
    "            a = np.random.randint(len(self.Q[s]))\n",
    "\n",
    "        return a\n",
    "\n",
    "    def get_parameters_exponential_decay(\n",
    "        self, initial_value=1, min_value=0.01, decay_rate=0.99\n",
    "    ):\n",
    "        num_points = self.number_of_iterations\n",
    "\n",
    "        exponential_decay_parameters = initial_value * (\n",
    "            decay_rate ** np.arange(num_points)\n",
    "        )\n",
    "        exponential_decay_parameters = np.where(\n",
    "            exponential_decay_parameters < min_value,\n",
    "            min_value,\n",
    "            exponential_decay_parameters,\n",
    "        )\n",
    "\n",
    "        return exponential_decay_parameters\n",
    "\n",
    "    def do_one_qlearning_iteration(self, iteration, γ=0.99):\n",
    "\n",
    "        s = (0, (0, 0, 0, 0, 0, 0, 0, 0, 0, 0))\n",
    "        terminated = 0\n",
    "        truncated = 0\n",
    "        s_prime = (0, (0, 0, 0, 0, 0, 0, 0, 0, 0, 0))\n",
    "\n",
    "        trajectory = []\n",
    "\n",
    "        model = PlantModel(10)\n",
    "        while not terminated and not truncated:\n",
    "            a = self.epsilon_greedy_exponential(iteration, s)\n",
    "            s_prime, R, terminated, truncated= model.step(a)\n",
    "\n",
    "\n",
    "            if s_prime not in self.Q:\n",
    "                self.Q[s_prime] = [0, 0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            self.Q[s][a] = self.Q[s][a] + self.α[iteration] * (\n",
    "                R + γ * max(self.Q[s_prime]) - self.Q[s][a]\n",
    "            )\n",
    "\n",
    "            trajectory.append((s, a, R, s_prime))\n",
    "\n",
    "            s = s_prime\n",
    "\n",
    "        return trajectory\n",
    "\n",
    "    def do_qlearning(self):\n",
    "        for i in tqdm(range(self.number_of_iterations)):\n",
    "            trajectory = self.do_one_qlearning_iteration(i)\n",
    "            self.trajectories.append(trajectory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "QL = QLearning(new_Q_Table, number_of_iterations=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [1:45:58<00:00, 157.28it/s] \n"
     ]
    }
   ],
   "source": [
    "QL.do_qlearning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_Table = QL.Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN THIS CELL\n",
    "# with open(\"Q_Table.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(Q_Table, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Q_Table.pkl\", \"rb\") as file:\n",
    "    loaded_Q_Table = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy(Q_Table):\n",
    "\n",
    "    policy = {}\n",
    "\n",
    "    for state in Q_Table.keys():\n",
    "        optimal_action = np.argmax(loaded_Q_Table[state])\n",
    "        policy[state] = optimal_action\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = get_policy(loaded_Q_Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DO NOT RUN THIS CELL\n",
    "# with open(\"Policy.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(policy, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Policy.pkl\", \"rb\") as file:\n",
    "    policy = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sum_of_reward_from_multiple_trajectories(policy, number_of_trajectories=1000):\n",
    "\n",
    "    sum_of_rewards = []\n",
    "\n",
    "    for _ in tqdm(range(number_of_trajectories)):\n",
    "\n",
    "        trajectory = generate_trajectory(policy)\n",
    "\n",
    "        rewards = list(map(lambda x: x[2], trajectory))\n",
    "        sum_of_rewards.append(sum(rewards))\n",
    "\n",
    "    return sum_of_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistics_of_total_reward(policy):\n",
    "    sum_of_rewards = get_sum_of_reward_from_multiple_trajectories(policy, 1000)\n",
    "    mean_reward = statistics.mean(sum_of_rewards)\n",
    "    std_dev = statistics.stdev(sum_of_rewards)\n",
    "\n",
    "    return mean_reward, std_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 447.43it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-109.715, 107.80823897732209)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_statistics_of_total_reward(policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
