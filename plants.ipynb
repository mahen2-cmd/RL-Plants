{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mesa\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Development of an agent-based simulation model in combination with reinforcement learning in Python using Mesa library\n",
    "\n",
    "\n",
    "\n",
    "# - At the beginning of an episode, 10 plants (as agents) are planted\n",
    "\n",
    "# - Plants must grow for 10 days (steps) before they can be harvested.\n",
    "\n",
    "# - Each plant has a 10% chance of dying every day.\n",
    "\n",
    "# - A new (fresh) plant can be bought every day (cost $10) to be planted\n",
    "\n",
    "# - The aim is to harvest 10 plants that each grew for 10 days. When the goal is reached, there is a reward of $20 per plant harvested and the episode ends\n",
    "\n",
    "# - Each day of the episode costs $5\n",
    "\n",
    "# - Reinforcement learning is now used to find a strategy when to plant new trees that minimizes total costs.\n",
    "\n",
    "\n",
    "# UPDATES\n",
    "\n",
    "# Just 2 things:\n",
    "\n",
    "\n",
    "\n",
    "# 1) May I ask you to include the possibility to run the simulation environment without the RL optimization. Following variables should be possible to be defined:\n",
    "# - a daily probability for the decision variable to buy or not to buy.\n",
    "# - number of episodes, the simulation will run\n",
    "# At the end of each episode, I'd like to see a table with the days (horizontal) and the plants (vertical) showing the different states of each plant on each day.\n",
    "# At the end of the entire simulation, I'd like to see a table showing important values of each episode, such as:\n",
    "# - the total costs\n",
    "# - the number of days needed\n",
    "\n",
    "\n",
    "\n",
    "# 2) May I also ask you to precisely comment the code, in order I can follow and understand it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlantAgent(mesa.Agent):\n",
    "\n",
    "    def __init__(self, unique_id, model):\n",
    "        super().__init__(unique_id, model)\n",
    "\n",
    "        self.alive = True\n",
    "        self.age = 0\n",
    "        self.harvested = False\n",
    "\n",
    "    def step(self):\n",
    "        if self.alive and self.age != 10:\n",
    "            self.age += 1\n",
    "\n",
    "        if self.age == 10:\n",
    "            self.harvested = True\n",
    "\n",
    "\n",
    "        if not self.alive or self.harvested:\n",
    "            return\n",
    "\n",
    "\n",
    "        death_outcome = random.choices([False, True], weights=[0.1, 0.9], k=1)[0]\n",
    "        self.alive = death_outcome\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlantModel(mesa.Model):\n",
    "\n",
    "    def __init__(self, N, render_mode=\"not human\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_agents = N\n",
    "        self.number_of_days = 0\n",
    "        self.number_of_plants_harvested = 0\n",
    "\n",
    "        self.complete_data = []\n",
    "        self.total_cost = 0\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        self.schedule = mesa.time.RandomActivation(self)\n",
    "\n",
    "        for i in range(self.num_agents):\n",
    "            a = PlantAgent(i, self)\n",
    "            self.schedule.add(a)\n",
    "\n",
    "    def get_state(self):\n",
    "        agents = [a for a in self.schedule.agents]\n",
    "        state = []\n",
    "\n",
    "        number_of_plants_harvested = 0\n",
    "\n",
    "        for a in agents:\n",
    "\n",
    "            if a.alive and not a.harvested:\n",
    "                state.append(a.age)\n",
    "\n",
    "            if a.harvested:\n",
    "                number_of_plants_harvested += 1\n",
    "\n",
    "        return number_of_plants_harvested, tuple(sorted(state))\n",
    "\n",
    "    def create_plant_data_string(self, plant_data):\n",
    "        plant_data_string = \"\"\n",
    "\n",
    "        if plant_data[1] == True:\n",
    "            plant_data_string += \"Alive\"\n",
    "        else:\n",
    "            plant_data_string += \"Dead\"\n",
    "\n",
    "        plant_data_string += \"   Age:\"\n",
    "        plant_data_string += str(plant_data[2])\n",
    "        plant_data_string += \"   \"\n",
    "\n",
    "        if plant_data[3] == True:\n",
    "            plant_data_string += \"Harvested\"\n",
    "        else:\n",
    "            plant_data_string += \"Not Harvested\"\n",
    "\n",
    "        return plant_data_string\n",
    "\n",
    "    def get_agent_data(self):\n",
    "\n",
    "        agents = [a for a in self.schedule.agents]\n",
    "\n",
    "        agent_data = []\n",
    "\n",
    "        for a in agents:\n",
    "\n",
    "            agent_data.append((a.unique_id, a.alive, a.age, a.harvested))\n",
    "\n",
    "        sorted_agent_data = sorted(agent_data, key=lambda x: x[0])\n",
    "\n",
    "        return sorted_agent_data\n",
    "\n",
    "    def prepare_data(self):\n",
    "        agent_data = self.get_agent_data()\n",
    "\n",
    "        data = [self.create_plant_data_string(d) for d in agent_data]\n",
    "        return data\n",
    "\n",
    "    def transform_complete_data(self):\n",
    "        columns = [\"Day \" + str(i+1) for i in range(self.number_of_days)]\n",
    "\n",
    "\n",
    "        df = pd.DataFrame(np.array(self.complete_data).T, columns=columns)\n",
    "        return df\n",
    "\n",
    "    def check_terminated(self, observation):\n",
    "        return list(map(lambda x: x[1] == 10, observation)).count(True)\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "\n",
    "        self.number_of_days += 1\n",
    "        terminated = 0\n",
    "        truncated = False\n",
    "        reward = -5\n",
    "        info = []\n",
    "\n",
    "        if action == 1:\n",
    "            self.num_agents += 1\n",
    "            a = PlantAgent(self.num_agents - 1, self)\n",
    "            self.schedule.add(a)\n",
    "\n",
    "            reward -= 10\n",
    "\n",
    "            if self.render_mode == \"human\":\n",
    "                for d in self.complete_data:\n",
    "                    d.append(\"Not Available\")\n",
    "\n",
    "        self.schedule.step()\n",
    "\n",
    "        observation = self.get_state()\n",
    "\n",
    "        difference = observation[0] - self.number_of_plants_harvested\n",
    "        reward += difference * 20\n",
    "\n",
    "        self.number_of_plants_harvested = observation[0]\n",
    "\n",
    "        if self.number_of_days == 500:\n",
    "            truncated = True\n",
    "\n",
    "        if self.number_of_plants_harvested == 10:\n",
    "            terminated = 1\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "\n",
    "            current_data = self.prepare_data()\n",
    "            self.complete_data.append(current_data)\n",
    "\n",
    "            info = self.transform_complete_data()\n",
    "\n",
    "            self.total_cost +=  -1* reward\n",
    "\n",
    "        return observation, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PlantModel(10, render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probabilistic_action():\n",
    "    BUY_PROB = 0.9\n",
    "    NOT_BUY_PROB = 0.1\n",
    "    action = random.choices([0, 1], weights=[NOT_BUY_PROB, BUY_PROB], k=1)[0]\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = probabilistic_action()\n",
    "observation, reward, terminated, truncated, info = model.step(action)\n",
    "\n",
    "if terminated or truncated:\n",
    "    print(\"Done\")\n",
    "    print(\"Total Cost is \", model.total_cost)\n",
    "    print(\"Total number of days is \", model.number_of_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thera are a few things to consider\n",
    "\n",
    "Terminal State: This would be achieved when we have 10 plants harvested.  \n",
    "Truncation: The environment would truncate in 500 steps.  \n",
    "Reward: Positive for harvesting and negative for buying a plant and on every step.  \n",
    "Observation: This would be the current state. The state would be the number of plants harvested and the age of each plant.  \n",
    "Info: This is the complete data. It includes the number of plants on the vertical and the number of days on the horizontal.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN THIS - THIS is JUST TO ESTIMATE THE STATE SPACE\n",
    "\n",
    "def get_state_space(number_of_iterations = 100000):\n",
    "\n",
    "    state_space = set()\n",
    "\n",
    "    truncated = 0\n",
    "    model = PlantModel(10)\n",
    "    initial_state = model.get_state()\n",
    "    state_space.add(initial_state)\n",
    "\n",
    "\n",
    "    for _ in tqdm(range(number_of_iterations)):\n",
    "\n",
    "        model = PlantModel(10)\n",
    "        observation = []\n",
    "        terminated = 0\n",
    "\n",
    "\n",
    "\n",
    "        while not terminated and not truncated:\n",
    "\n",
    "            action = random.randint(0, 1)\n",
    "            observation, _, terminated, truncated = model.step(action)\n",
    "            state_space.add(observation)\n",
    "\n",
    "    state_space = list(state_space)\n",
    "    state_space = sorted(state_space, key=lambda x: (x[0], x[1]))\n",
    "\n",
    "    return state_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN\n",
    "# state_space = get_state_space(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN THIS - THIS IS JUST TO STORE THE STATE SPACE\n",
    "\n",
    "\n",
    "# with open(\"state_space.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(state_space, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"state_space.pkl\", \"rb\") as file:\n",
    "    state_space = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_Q_Table(state_space):\n",
    "    Q_Table = {}\n",
    "\n",
    "    for s in state_space:\n",
    "        Q_Table[s] = [0, 0]\n",
    "\n",
    "    return Q_Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_Table = initialize_Q_Table(state_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trajectory(policy):\n",
    "\n",
    "    trajectory = []\n",
    "\n",
    "    state = (0, (0, 0, 0, 0, 0, 0, 0, 0, 0, 0))\n",
    "    terminated = 0\n",
    "    truncated = 0\n",
    "    next_state = (0, (0, 0, 0, 0, 0, 0, 0, 0, 0, 0))\n",
    "\n",
    "    model = PlantModel(10)\n",
    "\n",
    "    while not terminated and not truncated:\n",
    "        state = next_state\n",
    "        action = policy[state]\n",
    "\n",
    "        observation, reward, terminated, truncated, _ = model.step(action)\n",
    "\n",
    "        next_state = observation\n",
    "        experience_tuple = (state, action, reward, next_state)\n",
    "\n",
    "        trajectory.append(experience_tuple)\n",
    "\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning:\n",
    "    def __init__(self, Q_Table, gamma=1, number_of_iterations=100000):\n",
    "        self.number_of_iterations = number_of_iterations\n",
    "\n",
    "        self.Q = Q_Table\n",
    "\n",
    "        self.ε = self.get_parameters_exponential_decay(decay_rate=0.999999)\n",
    "        self.α = self.get_parameters_exponential_decay(decay_rate=0.999999)\n",
    "\n",
    "        self.trajectories = [[]]\n",
    "\n",
    "    def epsilon_greedy_exponential(self, iteration, s):\n",
    "        ε = self.ε[iteration]\n",
    "\n",
    "        a = 0\n",
    "\n",
    "        if s not in self.Q:\n",
    "            self.Q[s] = [0, 0]\n",
    "\n",
    "        if np.random.random() > ε:\n",
    "            a = np.argmax(self.Q[s])\n",
    "        else:\n",
    "            a = np.random.randint(len(self.Q[s]))\n",
    "\n",
    "        return a\n",
    "\n",
    "    def get_parameters_exponential_decay(\n",
    "        self, initial_value=1, min_value=0.01, decay_rate=0.99\n",
    "    ):\n",
    "        num_points = self.number_of_iterations\n",
    "\n",
    "        exponential_decay_parameters = initial_value * (\n",
    "            decay_rate ** np.arange(num_points)\n",
    "        )\n",
    "        exponential_decay_parameters = np.where(\n",
    "            exponential_decay_parameters < min_value,\n",
    "            min_value,\n",
    "            exponential_decay_parameters,\n",
    "        )\n",
    "\n",
    "        return exponential_decay_parameters\n",
    "\n",
    "    def do_one_qlearning_iteration(self, iteration, γ=0.99):\n",
    "\n",
    "        s = (0, (0, 0, 0, 0, 0, 0, 0, 0, 0, 0))\n",
    "        terminated = 0\n",
    "        truncated = 0\n",
    "        s_prime = (0, (0, 0, 0, 0, 0, 0, 0, 0, 0, 0))\n",
    "\n",
    "        trajectory = []\n",
    "\n",
    "        model = PlantModel(10)\n",
    "        while not terminated and not truncated:\n",
    "            a = self.epsilon_greedy_exponential(iteration, s)\n",
    "            s_prime, R, terminated, truncated, _ = model.step(a)\n",
    "\n",
    "            if s_prime not in self.Q:\n",
    "                self.Q[s_prime] = [0, 0]\n",
    "\n",
    "            self.Q[s][a] = self.Q[s][a] + self.α[iteration] * (\n",
    "                R + γ * max(self.Q[s_prime]) - self.Q[s][a]\n",
    "            )\n",
    "\n",
    "            trajectory.append((s, a, R, s_prime))\n",
    "\n",
    "            s = s_prime\n",
    "\n",
    "        return trajectory\n",
    "\n",
    "    def do_qlearning(self):\n",
    "        for i in tqdm(range(4792269, self.number_of_iterations)):\n",
    "            trajectory = self.do_one_qlearning_iteration(i)\n",
    "            self.trajectories.append(trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "QL = QLearning(Q_Table, number_of_iterations=5000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4792269"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2908024 + 1884245"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207731/207731 [26:29<00:00, 130.66it/s]\n"
     ]
    }
   ],
   "source": [
    "QL.do_qlearning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_Table = QL.Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20804"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Q_Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN THIS CELL\n",
    "with open(\"Q_Table2.pkl\", \"wb\") as file:\n",
    "    pickle.dump(Q_Table, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Q_Table2.pkl\", \"rb\") as file:\n",
    "    Q_Table = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy(Q_Table):\n",
    "\n",
    "    policy = {}\n",
    "\n",
    "    for state in Q_Table.keys():\n",
    "        optimal_action = np.argmax(Q_Table[state])\n",
    "        policy[state] = optimal_action\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = get_policy(Q_Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DO NOT RUN THIS CELL\n",
    "with open(\"Policy.pkl\", \"wb\") as file:\n",
    "    pickle.dump(policy, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Policy.pkl\", \"rb\") as file:\n",
    "    policy = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sum_of_reward_from_multiple_trajectories(policy, number_of_trajectories=1000):\n",
    "\n",
    "    sum_of_rewards = []\n",
    "\n",
    "    for _ in tqdm(range(number_of_trajectories)):\n",
    "\n",
    "        trajectory = generate_trajectory(policy)\n",
    "\n",
    "        rewards = list(map(lambda x: x[2], trajectory))\n",
    "        sum_of_rewards.append(sum(rewards))\n",
    "\n",
    "    return sum_of_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistics_of_total_reward(policy):\n",
    "    sum_of_rewards = get_sum_of_reward_from_multiple_trajectories(policy, 1000)\n",
    "    mean_reward = statistics.mean(sum_of_rewards)\n",
    "    std_dev = statistics.stdev(sum_of_rewards)\n",
    "\n",
    "    return mean_reward, std_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:04<00:00, 205.33it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-107.595, 100.72654472288728)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_statistics_of_total_reward(policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
