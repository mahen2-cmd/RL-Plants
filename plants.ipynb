{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Development of an agent-based simulation model in combination with reinforcement learning in Python using Mesa library\n",
    "\n",
    "\n",
    "\n",
    "# - At the beginning of an episode, 10 plants (as agents) are planted\n",
    "\n",
    "# - Plants must grow for 10 days (steps) before they can be harvested.\n",
    "\n",
    "# - Each plant has a 10% chance of dying every day.\n",
    "\n",
    "# - A new (fresh) plant can be bought every day (cost $10) to be planted\n",
    "\n",
    "# - The aim is to harvest 10 plants that each grew for 10 days. When the goal is reached, there is a reward of $20 per plant harvested and the episode ends\n",
    "\n",
    "# - Each day of the episode costs $5\n",
    "\n",
    "# - Reinforcement learning is now used to find a strategy when to plant new trees that minimizes total costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mesa\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlantAgent(mesa.Agent):\n",
    "\n",
    "    def __init__(self, unique_id, model):\n",
    "        super().__init__(unique_id, model)\n",
    "\n",
    "        self.alive = True\n",
    "\n",
    "        self.age = 0\n",
    "        self.harvested = False\n",
    "\n",
    "    def step(self):\n",
    "        if self.alive:\n",
    "            if self.age != 10:\n",
    "                self.age += 1\n",
    "\n",
    "        if self.age == 10:\n",
    "            self.harvested = True\n",
    "\n",
    "        # print(f\"Agent ID: {str(self.unique_id)} Agent Status: {self.alive} Agent Life: {str(self.age)} Harvest Status: {self.harvested}\")\n",
    "\n",
    "        if not self.alive or self.harvested:\n",
    "            return\n",
    "\n",
    "\n",
    "        death_outcome = random.choices([False, True], weights=[0.1, 0.9], k=1)[0]\n",
    "\n",
    "        self.alive = death_outcome\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlantModel(mesa.Model):\n",
    "\n",
    "    def __init__(self, N):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_agents = N\n",
    "\n",
    "        self.number_of_days = 0\n",
    "        self.number_of_plants_harvested = 0\n",
    "\n",
    "        self.schedule = mesa.time.RandomActivation(self)\n",
    "\n",
    "        for i in range(self.num_agents):\n",
    "            a = PlantAgent(i, self)\n",
    "            self.schedule.add(a)\n",
    "\n",
    "    def get_state(self):\n",
    "        agents = [a for a in self.schedule.agents]\n",
    "        # state = [(0, 0) for i in range(len(agents))]\n",
    "        state = []\n",
    "\n",
    "        number_of_plants_harvested = 0\n",
    "\n",
    "        for a in agents:\n",
    "            # print(\n",
    "            #     f\"Agent ID: {str(a.unique_id)}     Agent Status: {a.alive}     Agent Life: {str(a.age)}     Harvest Status: {a.harvested}\"\n",
    "            # )\n",
    "\n",
    "            if a.alive and not a.harvested:\n",
    "                state.append(a.age)\n",
    "\n",
    "            if a.harvested:\n",
    "                number_of_plants_harvested += 1\n",
    "\n",
    "            # state[a.unique_id] = (a.alive, a.age)\n",
    "\n",
    "        return number_of_plants_harvested, tuple(sorted(state))\n",
    "        # return state\n",
    "\n",
    "    def check_terminated(self, observation):\n",
    "        return list(map(lambda x: x[1] == 10, observation)).count(True)\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        self.number_of_days += 1\n",
    "        terminated = 0\n",
    "        truncated = False\n",
    "        reward = -5\n",
    "\n",
    "        if action == 1:\n",
    "            self.num_agents += 1\n",
    "            a = PlantAgent(self.num_agents - 1, self)\n",
    "            self.schedule.add(a)\n",
    "\n",
    "            reward -= 10\n",
    "\n",
    "        self.schedule.step()\n",
    "\n",
    "        observation = self.get_state()\n",
    "\n",
    "\n",
    "        # number_of_plants_harvested = list(map(lambda x: x[0] and x[1] == 10, observation)).count(True)\n",
    "        difference = observation[0] - self.number_of_plants_harvested\n",
    "        reward += difference * 20\n",
    "\n",
    "        self.number_of_plants_harvested = observation[0]\n",
    "\n",
    "        if self.number_of_days == 500:\n",
    "            truncated = True\n",
    "\n",
    "        if self.number_of_plants_harvested == 10:\n",
    "            terminated = 1\n",
    "\n",
    "        return observation, reward, terminated, truncated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thera are a few things to consider\n",
    "\n",
    "Terminal State: This would be achieved when we have 10 plants  \n",
    "Truncation: It should eventually converge in 500 iterations. No more than 500 steps.  \n",
    "Reward: Positive for harvesting and negative for buying a plant plus on each step you get a negative reward  \n",
    "Observation: This would be the current state. The state would be a list of tuples (status_of_death, age)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essentially what you want the state space to be is that you want to know the status of each plant (alive, life, harvested) and the total cost so far\n",
    "\n",
    "# And, so we need to construct our state space.\n",
    "\n",
    "# There must be a function that constructs this state space for the model. The state space can also be infinite as we need to see what kind of things we have to make it in there.b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PlantModel(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = model.get_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, (0, 0, 0, 0, 0, 0, 0, 0, 0, 0))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_Table[initial_state] = [0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# for steps in range(500):\n",
    "observation, reward, terminated, truncated = model.step(0)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, [7]), 15, 1, False)"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation, reward, terminated, truncated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We want to define the state space.  \n",
    "2. We want to define the action space.  \n",
    "3. We want to make the Q and Value Table for each state. And, then do our reinforcement learning via value iteration, policy iteration or Q Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion is that we can't quite define the state and action space. They are in fact very hard to define and do. What we instead need is that a more nuanced approach to it. And, do Q Learning instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps we could try Monte Carlo Control to do it too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's figure out the state space first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500000/500000 [09:52<00:00, 843.54it/s]  \n"
     ]
    }
   ],
   "source": [
    "state_space = set()\n",
    "\n",
    "truncated = 0\n",
    "\n",
    "for _ in tqdm(range(500000)):\n",
    "    model = PlantModel(10)\n",
    "\n",
    "    observation = []\n",
    "    reward = 0\n",
    "    terminated = 0\n",
    "\n",
    "    while not terminated and not truncated:\n",
    "        action = random.randint(0, 1)\n",
    "\n",
    "        observation, reward, terminated, truncated = model.step(action)\n",
    "        # print()\n",
    "\n",
    "\n",
    "        state_space.add(observation)\n",
    "\n",
    "        # if observation not in state_space:\n",
    "        #     state_space.append((observation[0], tuple(observation[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"state_space.pkl\", \"wb\") as file:\n",
    "    pickle.dump(state_space_list, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"state_space.pkl\", \"rb\") as file:\n",
    "    loaded_state_space = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(loaded_state_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_Table = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in loaded_state_space:\n",
    "    Q_Table[state] = [0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_selection(state, epsilon=0.1):\n",
    "\n",
    "    if state not in Q_Table:\n",
    "        Q_Table[state] = [0, 0]\n",
    "\n",
    "\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, 1)\n",
    "    else:\n",
    "        return Q_Table[state].index(max(Q_Table[state]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_Table[sample_state] = [0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_state = (0, (1, 1, 1, 1))\n",
    "action_selection(sample_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trajectory(model, epsilon=0.1):\n",
    "    trajectory = []\n",
    "\n",
    "    trajectory = []\n",
    "\n",
    "    state = (0, (0, 0, 0, 0, 0, 0, 0, 0, 0, 0))\n",
    "    terminated = 0\n",
    "    truncated = 0\n",
    "    next_state = (0, (0, 0, 0, 0, 0, 0, 0, 0, 0, 0))\n",
    "\n",
    "    model = PlantModel(10)\n",
    "\n",
    "    while not terminated and not truncated:\n",
    "        state = next_state\n",
    "        action = action_selection(state, epsilon)\n",
    "\n",
    "        observation, reward, terminated, truncated = model.step(action)\n",
    "\n",
    "        next_state = observation\n",
    "\n",
    "        experience_tuple = (state, action, reward, next_state)\n",
    "\n",
    "        trajectory.append(experience_tuple)\n",
    "\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning:\n",
    "    def __init__(self, Q_Table, gamma=1, number_of_iterations=100000):\n",
    "        self.number_of_iterations = number_of_iterations\n",
    "\n",
    "        self.Q = Q_Table\n",
    "\n",
    "        self.ε = self.get_parameters_exponential_decay(decay_rate=0.99995)\n",
    "        self.α = self.get_parameters_exponential_decay(decay_rate=0.99995)\n",
    "\n",
    "        # self.trajectories = [[]]\n",
    "\n",
    "    def epsilon_greedy_exponential(self, iteration, s):\n",
    "        ε = self.ε[iteration]\n",
    "\n",
    "        a = 0\n",
    "\n",
    "        if s not in self.Q:\n",
    "            self.Q[s] = [0, 0]\n",
    "\n",
    "        if np.random.random() > ε:\n",
    "            a = np.argmax(self.Q[s])\n",
    "        else:\n",
    "            a = np.random.randint(len(self.Q[s]))\n",
    "\n",
    "        return a\n",
    "\n",
    "    def get_parameters_exponential_decay(\n",
    "        self, initial_value=1, min_value=0.01, decay_rate=0.99\n",
    "    ):\n",
    "        num_points = self.number_of_iterations\n",
    "\n",
    "        exponential_decay_parameters = initial_value * (\n",
    "            decay_rate ** np.arange(num_points)\n",
    "        )\n",
    "        exponential_decay_parameters = np.where(\n",
    "            exponential_decay_parameters < min_value,\n",
    "            min_value,\n",
    "            exponential_decay_parameters,\n",
    "        )\n",
    "\n",
    "        return exponential_decay_parameters\n",
    "\n",
    "    def do_one_qlearning_iteration(self, iteration, γ=0.99):\n",
    "\n",
    "        s = (0, (0, 0, 0, 0, 0, 0, 0, 0, 0, 0))\n",
    "        terminated = 0\n",
    "        truncated = 0\n",
    "        s_prime = (0, (0, 0, 0, 0, 0, 0, 0, 0, 0, 0))\n",
    "\n",
    "        trajectory = []\n",
    "\n",
    "        model = PlantModel(10)\n",
    "        while not terminated and not truncated:\n",
    "            a = self.epsilon_greedy_exponential(iteration, s)\n",
    "            s_prime, R, terminated, truncated= model.step(a)\n",
    "\n",
    "\n",
    "            if s_prime not in self.Q:\n",
    "                self.Q[s_prime] = [0, 0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            self.Q[s][a] = self.Q[s][a] + self.α[iteration] * (\n",
    "                R + γ * max(self.Q[s_prime]) - self.Q[s][a]\n",
    "            )\n",
    "\n",
    "            trajectory.append((s, a, R, s_prime))\n",
    "\n",
    "            s = s_prime\n",
    "\n",
    "        return trajectory\n",
    "\n",
    "    def do_qlearning(self):\n",
    "        for i in tqdm(range(self.number_of_iterations)):\n",
    "            # trajectory = self.do_one_qlearning_iteration(i)\n",
    "            self.do_one_qlearning_iteration(i)\n",
    "            # self.trajectories.append(trajectory)\n",
    "\n",
    "        # self.build_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "QL = QLearning(Q_Table, number_of_iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = QL.do_one_qlearning_iteration(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((0, (0, 0, 0, 0, 0, 0, 0, 0, 0, 0)), 0, -5, (0, (1, 1, 1, 1, 1, 1, 1, 1, 1)))\n",
      "((0, (1, 1, 1, 1, 1, 1, 1, 1, 1)), 0, -5, (0, (2, 2, 2, 2, 2, 2, 2)))\n",
      "((0, (2, 2, 2, 2, 2, 2, 2)), 0, -5, (0, (3, 3, 3, 3, 3)))\n",
      "((0, (3, 3, 3, 3, 3)), 0, -5, (0, (4, 4, 4, 4)))\n",
      "((0, (4, 4, 4, 4)), 0, -5, (0, (5, 5, 5)))\n",
      "((0, (5, 5, 5)), 1, -15, (0, (1, 6, 6)))\n",
      "((0, (1, 6, 6)), 1, -15, (0, (1, 2, 7)))\n",
      "((0, (1, 2, 7)), 0, -5, (0, (2, 3, 8)))\n",
      "((0, (2, 3, 8)), 1, -15, (0, (1, 4, 9)))\n",
      "((0, (1, 4, 9)), 0, 15, (1, (2, 5)))\n",
      "((1, (2, 5)), 0, -5, (1, (6,)))\n",
      "((1, (6,)), 1, -15, (1, (1, 7)))\n",
      "((1, (1, 7)), 0, -5, (1, (2, 8)))\n",
      "((1, (2, 8)), 0, -5, (1, (3, 9)))\n",
      "((1, (3, 9)), 0, 15, (2, (4,)))\n",
      "((2, (4,)), 1, -15, (2, (1, 5)))\n",
      "((2, (1, 5)), 1, -15, (2, (1, 2, 6)))\n",
      "((2, (1, 2, 6)), 0, -5, (2, (2, 3, 7)))\n",
      "((2, (2, 3, 7)), 1, -15, (2, (1, 3, 4, 8)))\n",
      "((2, (1, 3, 4, 8)), 1, -15, (2, (2, 4, 5, 9)))\n",
      "((2, (2, 4, 5, 9)), 0, 15, (3, (3, 5, 6)))\n",
      "((3, (3, 5, 6)), 1, -15, (3, (1, 4, 6, 7)))\n",
      "((3, (1, 4, 6, 7)), 0, -5, (3, (2, 5, 7, 8)))\n",
      "((3, (2, 5, 7, 8)), 0, -5, (3, (3, 6, 8, 9)))\n",
      "((3, (3, 6, 8, 9)), 1, 5, (4, (1, 4, 7, 9)))\n",
      "((4, (1, 4, 7, 9)), 1, 5, (5, (1, 2, 5, 8)))\n",
      "((5, (1, 2, 5, 8)), 0, -5, (5, (2, 3, 9)))\n",
      "((5, (2, 3, 9)), 1, 5, (6, (1, 3, 4)))\n",
      "((6, (1, 3, 4)), 0, -5, (6, (2, 4, 5)))\n",
      "((6, (2, 4, 5)), 0, -5, (6, (3, 5, 6)))\n",
      "((6, (3, 5, 6)), 1, -15, (6, (1, 4, 6, 7)))\n",
      "((6, (1, 4, 6, 7)), 0, -5, (6, (2, 5, 7, 8)))\n",
      "((6, (2, 5, 7, 8)), 1, -15, (6, (1, 3, 8, 9)))\n",
      "((6, (1, 3, 8, 9)), 0, 15, (7, (2, 4, 9)))\n",
      "((7, (2, 4, 9)), 1, 5, (8, (1, 3, 5)))\n",
      "((8, (1, 3, 5)), 1, -15, (8, (1, 2, 4, 6)))\n",
      "((8, (1, 2, 4, 6)), 0, -5, (8, (2, 3, 5, 7)))\n",
      "((8, (2, 3, 5, 7)), 0, -5, (8, (3, 4, 6, 8)))\n",
      "((8, (3, 4, 6, 8)), 1, -15, (8, (1, 4, 5, 9)))\n",
      "((8, (1, 4, 5, 9)), 1, 5, (9, (1, 5, 6)))\n",
      "((9, (1, 5, 6)), 0, -5, (9, (6, 7)))\n",
      "((9, (6, 7)), 0, -5, (9, (7, 8)))\n",
      "((9, (7, 8)), 1, -15, (9, (1, 8, 9)))\n",
      "((9, (1, 8, 9)), 1, 5, (10, (1, 2, 9)))\n"
     ]
    }
   ],
   "source": [
    "for experience in trajectory:\n",
    "    print(experience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_Q_Table = QL.Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, (3, 3, 3, 3, 3))"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectory[3][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, -15.0]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_Q_Table[trajectory[5][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = generate_trajectory(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((0, (0, 0, 0, 0, 0, 0, 0, 0, 0, 0)),\n",
       "  0,\n",
       "  -5,\n",
       "  (0, (1, 1, 1, 1, 1, 1, 1, 1, 1, 1))),\n",
       " ((0, (1, 1, 1, 1, 1, 1, 1, 1, 1, 1)),\n",
       "  0,\n",
       "  -5,\n",
       "  (0, (2, 2, 2, 2, 2, 2, 2, 2, 2, 2))),\n",
       " ((0, (2, 2, 2, 2, 2, 2, 2, 2, 2, 2)),\n",
       "  0,\n",
       "  -5,\n",
       "  (0, (3, 3, 3, 3, 3, 3, 3, 3, 3))),\n",
       " ((0, (3, 3, 3, 3, 3, 3, 3, 3, 3)), 0, -5, (0, (4, 4, 4, 4, 4, 4, 4, 4))),\n",
       " ((0, (4, 4, 4, 4, 4, 4, 4, 4)), 1, -15, (0, (5, 5, 5, 5, 5, 5, 5))),\n",
       " ((0, (5, 5, 5, 5, 5, 5, 5)), 0, -5, (0, (6, 6, 6, 6, 6, 6, 6))),\n",
       " ((0, (6, 6, 6, 6, 6, 6, 6)), 0, -5, (0, (7, 7, 7, 7, 7, 7, 7))),\n",
       " ((0, (7, 7, 7, 7, 7, 7, 7)), 0, -5, (0, (8, 8, 8, 8, 8, 8, 8))),\n",
       " ((0, (8, 8, 8, 8, 8, 8, 8)), 0, -5, (0, (9, 9, 9, 9, 9, 9, 9))),\n",
       " ((0, (9, 9, 9, 9, 9, 9, 9)), 0, 135, (7, ())),\n",
       " ((7, ()), 0, -5, (7, ())),\n",
       " ((7, ()), 0, -5, (7, ())),\n",
       " ((7, ()), 0, -5, (7, ())),\n",
       " ((7, ()), 1, -15, (7, (1,))),\n",
       " ((7, (1,)), 0, -5, (7, ())),\n",
       " ((7, ()), 0, -5, (7, ())),\n",
       " ((7, ()), 0, -5, (7, ())),\n",
       " ((7, ()), 0, -5, (7, ())),\n",
       " ((7, ()), 0, -5, (7, ())),\n",
       " ((7, ()), 0, -5, (7, ())),\n",
       " ((7, ()), 0, -5, (7, ())),\n",
       " ((7, ()), 0, -5, (7, ())),\n",
       " ((7, ()), 0, -5, (7, ())),\n",
       " ((7, ()), 0, -5, (7, ())),\n",
       " ((7, ()), 0, -5, (7, ())),\n",
       " ((7, ()), 1, -15, (7, (1,))),\n",
       " ((7, (1,)), 0, -5, (7, (2,))),\n",
       " ((7, (2,)), 0, -5, (7, (3,))),\n",
       " ((7, (3,)), 0, -5, (7, (4,))),\n",
       " ((7, (4,)), 0, -5, (7, (5,))),\n",
       " ((7, (5,)), 1, -15, (7, (1, 6))),\n",
       " ((7, (1, 6)), 0, -5, (7, (2, 7))),\n",
       " ((7, (2, 7)), 0, -5, (7, (8,))),\n",
       " ((7, (8,)), 0, -5, (7, (9,))),\n",
       " ((7, (9,)), 0, 15, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 1, -15, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 1, -15, (8, (1,))),\n",
       " ((8, (1,)), 0, -5, (8, (2,))),\n",
       " ((8, (2,)), 0, -5, (8, (3,))),\n",
       " ((8, (3,)), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 0, -5, (8, ())),\n",
       " ((8, ()), 1, -15, (8, (1,))),\n",
       " ((8, (1,)), 0, -5, (8, (2,))),\n",
       " ((8, (2,)), 0, -5, (8, (3,))),\n",
       " ((8, (3,)), 0, -5, (8, (4,))),\n",
       " ((8, (4,)), 0, -5, (8, (5,))),\n",
       " ((8, (5,)), 0, -5, (8, (6,))),\n",
       " ((8, (6,)), 0, -5, (8, (7,))),\n",
       " ((8, (7,)), 0, -5, (8, (8,))),\n",
       " ((8, (8,)), 0, -5, (8, (9,))),\n",
       " ((8, (9,)), 0, 15, (9, ())),\n",
       " ((9, ()), 0, -5, (9, ())),\n",
       " ((9, ()), 0, -5, (9, ())),\n",
       " ((9, ()), 0, -5, (9, ())),\n",
       " ((9, ()), 0, -5, (9, ())),\n",
       " ((9, ()), 0, -5, (9, ())),\n",
       " ((9, ()), 0, -5, (9, ())),\n",
       " ((9, ()), 0, -5, (9, ())),\n",
       " ((9, ()), 0, -5, (9, ())),\n",
       " ((9, ()), 0, -5, (9, ())),\n",
       " ((9, ()), 0, -5, (9, ())),\n",
       " ((9, ()), 0, -5, (9, ())),\n",
       " ((9, ()), 0, -5, (9, ())),\n",
       " ((9, ()), 0, -5, (9, ())),\n",
       " ((9, ()), 0, -5, (9, ())),\n",
       " ((9, ()), 0, -5, (9, ())),\n",
       " ((9, ()), 0, -5, (9, ())),\n",
       " ((9, ()), 0, -5, (9, ())),\n",
       " ((9, ()), 0, -5, (9, ())),\n",
       " ((9, ()), 0, -5, (9, ())),\n",
       " ((9, ()), 0, -5, (9, ())),\n",
       " ((9, ()), 0, -5, (9, ())),\n",
       " ((9, ()), 0, -5, (9, ())),\n",
       " ((9, ()), 0, -5, (9, ())),\n",
       " ((9, ()), 0, -5, (9, ())),\n",
       " ((9, ()), 0, -5, (9, ())),\n",
       " ((9, ()), 0, -5, (9, ())),\n",
       " ((9, ()), 0, -5, (9, ())),\n",
       " ((9, ()), 0, -5, (9, ())),\n",
       " ((9, ()), 0, -5, (9, ())),\n",
       " ((9, ()), 1, -15, (9, (1,))),\n",
       " ((9, (1,)), 0, -5, (9, (2,))),\n",
       " ((9, (2,)), 0, -5, (9, (3,))),\n",
       " ((9, (3,)), 0, -5, (9, (4,))),\n",
       " ((9, (4,)), 0, -5, (9, (5,))),\n",
       " ((9, (5,)), 0, -5, (9, (6,))),\n",
       " ((9, (6,)), 0, -5, (9, (7,))),\n",
       " ((9, (7,)), 0, -5, (9, (8,))),\n",
       " ((9, (8,)), 0, -5, (9, (9,))),\n",
       " ((9, (9,)), 1, 5, (10, ()))]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
